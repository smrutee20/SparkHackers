{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import ImageDatasets\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, person_dir, clothes_dir, transform=None):\n",
    "        self.person_dir = person_dir\n",
    "        self.clothes_dir = clothes_dir\n",
    "        self.transform = transform\n",
    "        self.person_images = os.listdir(person_dir)\n",
    "        self.clothes_images = os.listdir(clothes_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.person_images), len(self.clothes_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        person_img_path = os.path.join(self.person_dir, self.person_images[idx])\n",
    "        clothes_img_path = os.path.join(self.clothes_dir, self.clothes_images[idx])\n",
    "        \n",
    "        person_image = Image.open(person_img_path).convert('RGB')\n",
    "        clothes_image = Image.open(clothes_img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            person_image = self.transform(person_image)\n",
    "            clothes_image = self.transform(clothes_image)\n",
    "        \n",
    "        return person_image, clothes_image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 192)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = FashionDataset('ImageDataset/train/person', 'ImageDataset/train/cloth', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = FashionDataset('ImageDataset/test/person', 'ImageDataset/test/cloth', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleUNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleUNet()\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for person_images, clothes_images in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(person_images)\n",
    "        loss = criterion(outputs, person_images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for person_images, clothes_images in test_loader:\n",
    "        outputs = model(person_images)\n",
    "        loss = criterion(outputs, person_images)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "\n",
    "# Pose detection with cvzone\n",
    "pose_detector = PoseDetector()\n",
    "\n",
    "def get_keypoints(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    keypoints, _ = pose_detector.findPose(image)\n",
    "    return keypoints\n",
    "\n",
    "def align_clothes(person_image, clothes_image, keypoints):\n",
    "    # Example alignment logic\n",
    "    # For simplicity, we'll use a placeholder\n",
    "    return clothes_image\n",
    "\n",
    "def test_virtual_try_on_with_pose(person_image_path, clothes_image_path):\n",
    "    model.eval()\n",
    "    \n",
    "    person_image = Image.open(person_image_path).convert('RGB')\n",
    "    clothes_image = Image.open(clothes_image_path).convert('RGB')\n",
    "    \n",
    "    keypoints = get_keypoints(person_image_path)\n",
    "    if keypoints is None:\n",
    "        print(\"No keypoints detected.\")\n",
    "        return\n",
    "    \n",
    "    aligned_clothes_image = align_clothes(person_image, clothes_image, keypoints)\n",
    "    \n",
    "    person_tensor = transform(person_image).unsqueeze(0)\n",
    "    clothes_tensor = transform(aligned_clothes_image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(person_tensor)\n",
    "    \n",
    "    output_image = transforms.ToPILImage()(output.squeeze())\n",
    "    plt.imshow(output_image)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "test_virtual_try_on_with_pose('ImageDataset/test/person/example_person.jpg', 'ImageDataset/test/cloth/example_cloth.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cvzone\n",
      "  Downloading cvzone-1.6.1.tar.gz (25 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: opencv-python in ./.venv/lib/python3.12/site-packages (from cvzone) (4.10.0.84)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from cvzone) (2.0.0)\n",
      "Building wheels for collected packages: cvzone\n",
      "  Building wheel for cvzone (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cvzone: filename=cvzone-1.6.1-py3-none-any.whl size=26295 sha256=5db4e6e1fa0b0096b3d6a1fe044f9521a174491dc059b9f678aca3a609d85474\n",
      "  Stored in directory: /home/raimudit2003/.cache/pip/wheels/5d/21/e8/3147ae88d44e27f06e0175d337a7673c70fb957202cbbe2034\n",
      "Successfully built cvzone\n",
      "Installing collected packages: cvzone\n",
      "Successfully installed cvzone-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cvzone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
